{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing inference with SageMaker Built-in Object Detection model\n",
    "\n",
    "1. [Download the trained model artifact](#download)\n",
    "1. [Convert training model to deployable model](#convert)\n",
    "1. [Inference](#inference)\n",
    "  1. [model load](#load)\n",
    "  1. [single image inference](#singleinference)\n",
    "  1. [batch inference](#batchinference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install gluoncv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from gluoncv.utils import download, viz\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import json\n",
    "import boto3\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the trained model artifact <a id='download'></a>\n",
    "\n",
    "The trained model parameters along with its network definition is stored in a tar.gz file in the output path for the training job. We need to download and unzip it to local disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_ID='od-demo-2020-01-24-05-26-12' \n",
    "S3_OUTPUT_BUCKET = 'robcost-potatohead'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client =  boto3.client('sagemaker')\n",
    "MODEL_ARTIFACT = sagemaker_client.describe_training_job(TrainingJobName=JOB_ID)['ModelArtifacts']['S3ModelArtifacts']\n",
    "MODEL_ARTIFACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def make_tmp_folder(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except OSError as e:\n",
    "        print(\"{} folder already exists\".format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_FOLDER = 'trained-model'\n",
    "make_tmp_folder(TMP_FOLDER)\n",
    "\n",
    "!aws s3 cp $MODEL_ARTIFACT $TMP_FOLDER/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzipping the model you will find three files in your directory:\n",
    "```\n",
    "model_algo_1-symbol.json   <-- neural network definition \n",
    "hyperparams.json           <-- hyper parameters  \n",
    "model_algo_1-0000.params   <-- trained weights for the neural network\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf $TMP_FOLDER/model.tar.gz -C $TMP_FOLDER/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the training model to a deployable model <a id='convert'></a>\n",
    "\n",
    "The model output produced by the built-in object detection model leaves the loss layer in place and does not include a [non-max suppression (NMS) layer](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH). To make it ready for inference on our machine, we need to remove the loss layer and add the NMS layer. We will be using a script from this GitHub repo: https://github.com/zhreshold/mxnet-ssd\n",
    "\n",
    "Make sure to clone this Git repo to your ~/SageMaker folder\n",
    "\n",
    "```\n",
    "cd ~/SageMaker\n",
    "git clone https://github.com/zhreshold/mxnet-ssd.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd ~/SageMaker\n",
    "git clone https://github.com/zhreshold/mxnet-ssd.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run the `deploy.py` script to convert a trained model to a deployable model. I  found that you must use python2 to run this script successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/ec2-user/SageMaker/mxnet-ssd/deploy.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this script, you need to make sure that command line options you pass in match exactly the hyperparameters of your training job. If youâ€™re unsure, refer the hyperparams.json file in your unpacked model artifacts to confirm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $TMP_FOLDER/hyperparams.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/ec2-user/SageMaker/mxnet-ssd/deploy.py --network resnet50 --num-class 1 --nms .45 --data-shape 512 --prefix $TMP_FOLDER/model_algo_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh $TMP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the copy of the deployable model artifact in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $TMP_FOLDER/deploy_model_algo_1-0000.params s3://$S3_OUTPUT_BUCKET/deployable-model/\n",
    "!aws s3 cp $TMP_FOLDER/deploy_model_algo_1-symbol.json s3://$S3_OUTPUT_BUCKET/deployable-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Doing inference with the model on local host <a id='inference'></a>\n",
    "\n",
    "Below code will run inference on a set of test images on the current notebook instance. Using a GPU instance (e.g. p2.\\*, p3.\\* family) will result in faster performance than CPU only instances. You can stop the SageMaker notebook instance and update the instance type, and restart the notebook instance before continuing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctx():\n",
    "    try:\n",
    "        gpus = mx.test_utils.list_gpus()\n",
    "        if len(gpus) > 0:\n",
    "            ctx = []\n",
    "            for gpu in gpus:\n",
    "                ctx.append(mx.gpu(gpu))\n",
    "        else:\n",
    "            ctx = [mx.cpu()]\n",
    "    except:\n",
    "        ctx = [mx.cpu()]\n",
    "    return ctx\n",
    "\n",
    "ctx = get_ctx()[0]\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = 512\n",
    "input_shapes=[('data', (1, 3, SHAPE, SHAPE))]\n",
    "confidence_threshold = 0.3\n",
    "CLASSES = ['potatohead']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A. Loading the model <a id=\"load\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_path=os.path.join(TMP_FOLDER, 'deploy_model_algo_1')\n",
    "print(\"param_path: {}\".format(param_path))\n",
    "sym, arg_params, aux_params = mx.model.load_checkpoint(param_path, 0)\n",
    "mod = mx.mod.Module(symbol=sym, label_names=[], context=ctx)\n",
    "mod.bind(for_training=False, data_shapes=input_shapes)\n",
    "mod.set_params(arg_params, aux_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Batch = namedtuple('Batch', ['data'])\n",
    "\n",
    "def predict_from_file(filepath, reshape=(SHAPE, SHAPE)):\n",
    "    # Switch RGB to BGR format (which ImageNet networks take)\n",
    "    img = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n",
    "    if img is None:\n",
    "        return []\n",
    "\n",
    "     # Resize image to fit network input\n",
    "    img = cv2.resize(img, reshape)\n",
    "    \n",
    "    org_image = img.copy()\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    img = np.swapaxes(img, 1, 2)\n",
    "    img = img[np.newaxis, :]\n",
    " \n",
    "    mod.forward(Batch([mx.nd.array(img)]))\n",
    "    prob = mod.get_outputs()[0].asnumpy()\n",
    "    prob = np.squeeze(prob)\n",
    "\n",
    "    return prob, org_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(image_path, threshold=confidence_threshold):\n",
    "    results, org_image = predict_from_file(image_path)\n",
    "    image_name = image_path.split(\"/\")[-1]\n",
    "    \n",
    "    filtered_result = results[results[:, 0] != -1]\n",
    "    filtered_result = filtered_result[filtered_result[:, 1] >=threshold]\n",
    "    \n",
    "    return filtered_result, org_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B. Test inference on single image <a id=\"singleinference\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://robcost-potatohead/dev/20191212_081328709_iOS.jpg ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prediction_image = '20191212_081328709_iOS.jpg'\n",
    "results, org_image = infer(prediction_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[:, (2, 4)] *= SHAPE\n",
    "results[:, (3, 5)] *= SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 10), facecolor='white', dpi=100)\n",
    "axis=axes\n",
    "\n",
    "ax = viz.plot_bbox(org_image, results[:,-4:], results[:,1], results[:,0],thresh=0.1, class_names=CLASSES, ax=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3C. Run inference on a batch of test images (~2 minutes on a p3.2xlarge instance, longer on cpu instances for 469 images) <a id=\"singleinference\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_IMAGE_FOLDER = 'validation'\n",
    "make_tmp_folder(VALIDATION_IMAGE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://greengrass-object-detection-blog/frames/validation_box_video/ $VALIDATION_IMAGE_FOLDER/ --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -1 $VALIDATION_IMAGE_FOLDER/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "img_list = os.listdir(VALIDATION_IMAGE_FOLDER)\n",
    "\n",
    "output_file = 'validation-inference-results.json'\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for img in img_list:\n",
    "        if img.endswith(\"jpg\"):\n",
    "            result,_orig = infer(os.path.join(VALIDATION_IMAGE_FOLDER, img))\n",
    "            to_write = {\"image\": img, \"prediction\": result.tolist()}\n",
    "            outfile.write(json.dumps(to_write))\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_pdf_name = 'validation-visualization.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "%run ./visualize_prediction_labels_batch.py -i $VALIDATION_IMAGE_FOLDER -l $output_file -f $visualization_pdf_name -c $confidence_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $visualization_pdf_name s3://$S3_OUTPUT_BUCKET/prediction-visualization/$JOB_ID/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open the [validation visualization pdf](./validation-visualization.pdf) on the notebook instance, or download the visualization PDF to your laptop to examine the prediction visualizaiton. e.g. \n",
    "```\n",
    "aws s3 cp s3://greengrass-object-detection-blog/prediction-visualization/od-demo-2019-08-01-04-57-12/validation-visualization.pdf .\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
