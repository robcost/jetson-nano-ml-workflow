{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Object Detection using the augmented manifest file format\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Specifying input Dataset](#Specifying-input-Dataset)\n",
    "4. [Training](#Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Object detection is the process of identifying and localizing objects in an image. A typical object detection solution takes in an image as input and provides a bounding box on the image where an object of interest is, along with identifying what object the box encapsulates. But before we have this solution, we need to process a training dataset, create and setup a training job for the algorithm so that the aglorithm can learn about the dataset and then host the algorithm as an endpoint, to which we can supply the query image.\n",
    "\n",
    "This notebook focuses on using the built-in SageMaker Single Shot multibox Detector ([SSD](https://arxiv.org/abs/1512.02325)) object detection algorithm to train model on your custom dataset. For dataset prepration or using the model for inference, please see other scripts in [this folder](./)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To train the Object Detection algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with we need an AWS account role with SageMaker access. This role is used to give SageMaker access to your data in S3. In this example, we will use the same role that was used to start this SageMaker notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = '<S3 Bucket Name>' # Valid name for S3 bucket.\n",
    "IMG_FOLDER = 'images' # Any valid S3 prefix.\n",
    "MANIFEST_FOLDER = 'manifest' # Any valid S3 prefix.\n",
    "CLASS_NAME = '<Target object label name>' # The single label that will be annotated in the Ground Truth job.\n",
    "OUTPUT_PREFIX = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "BUCKET = 'robcost-potato' # Valid name for S3 bucket.\n",
    "IMG_FOLDER = 'images' # Any valid S3 prefix.\n",
    "MANIFEST_FOLDER = 'manifest' # Any valid S3 prefix.\n",
    "CLASS_NAME = 'potatohead' # The single label that will be annotated in the Ground Truth job.\n",
    "OUTPUT_PREFIX = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that has the training manifests and will be used to store the tranied model artifacts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying input Dataset\n",
    "\n",
    "This notebook assumes you already have prepared two [Augmented Manifest Files](https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html) as training and validation input data for the object detection model.  \n",
    "\n",
    "There are many advantages to using **augmented manifest files** for your training input\n",
    "\n",
    "* No format conversion is required if you are using SageMaker Ground Truth to generate the data labels\n",
    "* Unlike the traditional approach of providing paths to the input images separately from its labels, augmented manifest file already combines both into one entry for each input image, reducing complexity in algorithm code for matching each image with labels. (Read this [blog post](https://aws.amazon.com/blogs/machine-learning/easily-train-models-using-datasets-labeled-by-amazon-sagemaker-ground-truth/) for more explanation.) \n",
    "* When splitting your dataset for train/validation/test, you don't need to rearrange and re-upload image files to different s3 prefixes for train vs validation. Once you upload your image files to S3, you never need to move it again. You can just place pointers to these images in your augmented manifest file for training and validation. More on the train/validation data split in this post later. \n",
    "* When using augmented manifest file, the training input images is loaded on to the training instance in *Pipe mode,* which means the input data is streamed directly to the training algorithm while it is running (vs. File mode, where all input files need to be downloaded to disk before the training starts). This results in faster training performance and less disk resource utilization. Read more in this [blog post](https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/) on the benefits of pipe mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data= \"s3://{}/{}/all_augmented.manifest\".format(BUCKET, CLASS_NAME)\n",
    "# s3_train_data= \"s3://{}/{}/train.manifest\".format(BUCKET, MANIFEST_FOLDER)\n",
    "s3_validation_data = \"s3://{}/{}/validation.manifest\".format(BUCKET, MANIFEST_FOLDER)\n",
    "print(\"Train data: {}\".format(s3_train_data) )\n",
    "print(\"Validation data: {}\".format(s3_validation_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = {\n",
    "    \"ChannelName\": \"train\",\n",
    "    \"InputMode\": \"Pipe\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"AugmentedManifestFile\",  \n",
    "            \"S3Uri\": s3_train_data,\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            # This must correspond to the JSON field names in your augmented manifest.\n",
    "            \"AttributeNames\": ['source-ref', CLASS_NAME]\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"application/x-recordio\",\n",
    "    \"RecordWrapperType\": \"RecordIO\",\n",
    "    \"CompressionType\": \"None\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input = {\n",
    "    \"ChannelName\": \"validation\",\n",
    "    \"InputMode\": \"Pipe\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"AugmentedManifestFile\",  \n",
    "            \"S3Uri\": s3_validation_data,\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "            #  This must correspond to the JSON field names in your augmented manifest.\n",
    "            \"AttributeNames\": ['source-ref', CLASS_NAME]\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"application/x-recordio\",\n",
    "    \"RecordWrapperType\": \"RecordIO\",\n",
    "    \"CompressionType\": \"None\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code computes the number of training samples, required in the training job request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "\n",
    "def read_manifest_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        output = [json.loads(line.strip()) for line in f.readlines()]\n",
    "        return output\n",
    "    \n",
    "!aws s3 cp $s3_train_data .    \n",
    "train_data = read_manifest_file(os.path.split(s3_train_data)[1])\n",
    "num_training_samples =  len(train_data)\n",
    "num_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_path = 's3://{}/{}'.format(BUCKET, OUTPUT_PREFIX)\n",
    "s3_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we are done with all the setup that is needed, we are ready to train our object detector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# This retrieves a docker container with the built in object detection SSD model. \n",
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'object-detection', repo_version='latest')\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a unique job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "job_name_prefix = CLASS_NAME.lower() + '-object-detection'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_job_name = job_name_prefix + timestamp\n",
    "model_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object detection algorithm at its core is the [Single-Shot Multi-Box detection algorithm (SSD)](https://arxiv.org/abs/1512.02325). This algorithm uses a `base_network`, which is typically a [VGG](https://arxiv.org/abs/1409.1556) or a [ResNet](https://arxiv.org/abs/1512.03385). (resnet is typically faster so for edge inferences, I'd recommend using this base network). The Amazon SageMaker object detection algorithm supports VGG-16 and ResNet-50 now. It also has a lot of options for hyperparameters that help configure the training job. The next step in our training, is to setup these hyperparameters and data channels for training the model. See the SageMaker Object Detection [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html) for more details on the hyperparameters.\n",
    "\n",
    "To figure out which works best for your data, run a hyperparameter tuning job. There's some example notebooks at [https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples) that you can use for reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where transfer learning happens. We use the pre-trained model and nuke the output layer by specifying\n",
    "# the num_classes value. You can also run a hyperparameter tuning job to figure out which values work the best. \n",
    "hyperparams = { \n",
    "            \"base_network\": 'resnet-50',\n",
    "            \"use_pretrained_model\": \"1\",\n",
    "            \"num_classes\": \"1\",   \n",
    "            \"mini_batch_size\": \"1\",\n",
    "            \"epochs\": \"30\",\n",
    "            \"learning_rate\": \"0.001\",\n",
    "            \"lr_scheduler_step\": \"10,20\",\n",
    "            \"lr_scheduler_factor\": \"0.25\",\n",
    "            \"optimizer\": \"sgd\",\n",
    "            \"momentum\": \"0.9\",\n",
    "            \"weight_decay\": \"0.0005\",\n",
    "            \"overlap_threshold\": \"0.5\",\n",
    "            \"nms_threshold\": \"0.45\",\n",
    "            \"image_shape\": \"512\",\n",
    "            \"label_width\": \"150\",\n",
    "            \"num_training_samples\": str(num_training_samples)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameters are set up, we configure the rest of the training job parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = \\\n",
    "    {\n",
    "        \"AlgorithmSpecification\": {\n",
    "            \"TrainingImage\": training_image,\n",
    "            \"TrainingInputMode\": \"Pipe\"\n",
    "        },\n",
    "        \"RoleArn\": role,\n",
    "        \"OutputDataConfig\": {\n",
    "            \"S3OutputPath\": s3_output_path\n",
    "        },\n",
    "        \"ResourceConfig\": {\n",
    "            \"InstanceCount\": 1,\n",
    "            \"InstanceType\": \"ml.p2.xlarge\",\n",
    "            \"VolumeSizeInGB\": 200\n",
    "        },\n",
    "        \"TrainingJobName\": model_job_name,\n",
    "        \"HyperParameters\": hyperparams,\n",
    "        \"StoppingCondition\": {\n",
    "            \"MaxRuntimeInSeconds\": 86400\n",
    "        },\n",
    "        \"InputDataConfig\": [\n",
    "            train_input,\n",
    "            validation_input\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='sagemaker')\n",
    "client.create_training_job(**training_params)\n",
    "\n",
    "# Confirm that the training job has started\n",
    "status = client.describe_training_job(TrainingJobName=model_job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the progess of the training job, you can repeatedly evaluate the following cell. When the training job status reads 'Completed', move on to the next part of the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='sagemaker')\n",
    "print(\"Training job status: \", client.describe_training_job(TrainingJobName=model_job_name)['TrainingJobStatus'])\n",
    "print(\"Secondary status: \", client.describe_training_job(TrainingJobName=model_job_name)['SecondaryStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do not continue until the job has completed**\n",
    "\n",
    "--------------\n",
    "\n",
    "## Once complete get the path to the generated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='sagemaker')\n",
    "s3_model_artifacts = client.describe_training_job(TrainingJobName='model_job_name')['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('Model Artifacts: ' + s3_model_artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Deployable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def make_tmp_folder(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name)\n",
    "    except OSError as e:\n",
    "        print(\"{} folder already exists\".format(folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_FOLDER = 'trained-model'\n",
    "make_tmp_folder(TMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $s3_model_artifacts $TMP_FOLDER/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf $TMP_FOLDER/model.tar.gz -C $TMP_FOLDER/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model output produced by the built-in object detection model leaves the loss layer in place and does not include a non-max suppression (NMS) layer. To make it ready for inference on our machine, we need to remove the loss layer and add the NMS layer. We will be using a script from this GitHub repo: https://github.com/zhreshold/mxnet-ssd\n",
    "\n",
    "Make sure to clone this Git repo to your ~/SageMaker folder\n",
    "\n",
    "cd ~/SageMaker\n",
    "git clone https://github.com/zhreshold/mxnet-ssd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd ~/SageMaker\n",
    "git clone https://github.com/zhreshold/mxnet-ssd.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install gluoncv\n",
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from gluoncv.utils import download, viz\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import json\n",
    "import boto3\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure the conversion script will work. If this errors then check the notebook is running with the conda_mxnet_p27 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ~/SageMaker/mxnet-ssd/deploy.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current hyperparameter settings for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"label_width\": \"150\", \"early_stopping_min_epochs\": \"10\", \"epochs\": \"30\", \"overlap_threshold\": \"0.5\", \"lr_scheduler_factor\": \"0.25\", \"_num_kv_servers\": \"auto\", \"weight_decay\": \"0.0005\", \"mini_batch_size\": \"1\", \"use_pretrained_model\": \"1\", \"freeze_layer_pattern\": \"\", \"lr_scheduler_step\": \"10,20\", \"early_stopping\": \"False\", \"early_stopping_patience\": \"5\", \"momentum\": \"0.9\", \"num_training_samples\": \"160\", \"optimizer\": \"sgd\", \"_tuning_objective_metric\": \"\", \"early_stopping_tolerance\": \"0.0\", \"learning_rate\": \"0.001\", \"kv_store\": \"device\", \"nms_threshold\": \"0.45\", \"num_classes\": \"1\", \"base_network\": \"resnet-50\", \"nms_topk\": \"400\", \"_kvstore\": \"device\", \"image_shape\": \"512\"}"
     ]
    }
   ],
   "source": [
    "!cat $TMP_FOLDER/hyperparams.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute script to remove loss layer and add NMS layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /home/ec2-user/SageMaker/mxnet-ssd/deploy.py --network resnet50 --num-class 1 --nms .45 --data-shape 512 --prefix $TMP_FOLDER/model_algo_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to confirm the new \"deployable\" params and symbol files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 306M\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K Apr 29 00:59 .\n",
      "drwxrwxr-x 4 ec2-user ec2-user 4.0K Apr 29 02:44 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 105M Apr 29 00:59 deploy_model_algo_1-0000.params\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 129K Apr 29 00:59 deploy_model_algo_1-symbol.json\n",
      "-rw-r--r-- 1 ec2-user ec2-user  677 Apr 28 14:52 hyperparams.json\n",
      "-rw-r--r-- 1 ec2-user ec2-user 105M Apr 28 14:52 model_algo_1-0000.params\n",
      "-rw-r--r-- 1 ec2-user ec2-user 130K Apr 28 14:52 model_algo_1-symbol.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  97M Apr 28 14:52 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!ls -alh $TMP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the deployable model files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file = '{}/deploy_model_algo_1-0000.params'.format(TMP_FOLDER)\n",
    "symbols_file = '{}/deploy_model_algo_1-symbol.json'.format(TMP_FOLDER)\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "with open(params_file) as file:\n",
    "    object = file.read()\n",
    "    s3_client.put_object(Body=object, Bucket=BUCKET, Key=OUTPUT_PREFIX + \"/deploy_model_algo_1-0000.params\")\n",
    "\n",
    "with open(symbols_file) as file:\n",
    "    object = file.read()\n",
    "    s3_client.put_object(Body=object, Bucket=BUCKET, Key=OUTPUT_PREFIX + \"/deploy_model_algo_1-symbol.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs required for next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training job completes, move on to the [next notebook](./03_local_inference_post_training.ipynb) to convert the trained model to a deployable format and run local inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
